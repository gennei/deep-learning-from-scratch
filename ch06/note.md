# 6章 学習に関するテクニック

## 6.1 パラメーターの更新
ニューラルネットワークの目的は損失関数の最小化。

### SGD
- 傾きを手がかりにするので、XY軸の一方方向に強い傾きがあるとそれに引っ張られる。

### Momentum
- ボールが地面を転がるような起動で最小化する。
- 物理の速度の概念を用いる。加速度を `1` より小さい値を設定し、徐々に動きがゆっくりになっていく。

### AdaGrad
- 最初は学習係数を大きく取り、学習が進むに連れ学習係数を小さくしていく
- パラメータの各要素ごとに学習し、更新する。

### Adam
- Momentum + AdaGrad
- 2015年に発表された手法
- 現在の DeepLearning でよく用いられる

## どれを使うか
ハイパーパラメータとの兼ね合いもあるのでいろいろ試す必要がある。


## 6.2 重みの初期値
- 重みの初期値の与え方は正しい学習を行う上で非常に重要である。
- 重みの初期値を0にしてしまうと、誤差逆伝播で各層の値が同一になってしまので、よくない。
- なので、ランダムな重み付けが必要である。

### Xavier の初期値
- 重みの初期値の設定
- 1/√n の標準偏差の分布を使う ※n:前層のノード数
- 活性化関数が線形であることが条件(sigmoidなど)
-

### He の初期値
- ReLU専用の初期値
- 前層のノード数がn個のとき、√(n/2) を標準偏差とするガウス分布を用いる

## 6.3 Batch Normalization

### メリット
- 学習を速くする
- 初期値にそれほど依存しない
- 過学習を抑制する

学習速度を速める。初期値に対してロバストになる。

## 6.4 正則化
過学習が起きる原因として主に2つ

- パラメータを大量に持ち、表現力が高いモデルである
- 訓練データが少ない

過学習を抑制するための技術として

- Weight decay
- Dropout

### Weight decay
大きな重みを持つもの対してペナルティを課す。

### Dropout
訓練時にニューロンをランダムに消去しながら学習する手法。

## 6.5 ハイパーパラメータの検証
テストデータを使用して、ハイパーパラメータの評価をしてはいけない。
ハイパーパラメータ検証用の検証用データを用意する必要がある。
訓練データの20％程度を検証用データとして使用する。

ハイパーパラメータの探索は値の範囲を徐々に絞り込むのがいい
